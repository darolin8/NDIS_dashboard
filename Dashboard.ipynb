{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVOhxQgWq6qzdNaz8uAVTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darolin8/NDIS_dashboard/blob/main/Dashboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "3hcNu1LW_pod",
        "outputId": "20b28ec5-2439-44a5-c47d-0a6df0e97835"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'streamlit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1672232294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_subplots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from scipy import stats\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, mean_absolute_error, accuracy_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import OneClassSVM\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Association rules imports\n",
        "try:\n",
        "    from mlxtend.frequent_patterns import apriori, association_rules\n",
        "    from mlxtend.preprocessing import TransactionEncoder\n",
        "    MLXTEND_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MLXTEND_AVAILABLE = False\n",
        "    st.warning(\"mlxtend not available. Install with: pip install mlxtend\")\n"
      ],
      "metadata": {
        "id": "KcGFa7hUApEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time series forecasting imports\n",
        "try:\n",
        "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "    from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "    STATSMODELS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    STATSMODELS_AVAILABLE = False\n",
        "    st.warning(\"statsmodels not available. Install with: pip install statsmodels\")\n"
      ],
      "metadata": {
        "id": "gZDPwVydAq0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NDIS color palette - defined at the top for global access\n",
        "NDIS_COLORS = {\n",
        "    'primary': '#003F5C',\n",
        "    'secondary': '#2F9E7D',\n",
        "    'accent': '#F59C2F',\n",
        "    'critical': '#DC2626',\n",
        "    'high': '#F59C2F',\n",
        "    'medium': '#2F9E7D',\n",
        "    'low': '#67A3C3',\n",
        "    'success': '#2F9E7D',\n",
        "    'warning': '#F59C2F',\n",
        "    'error': '#DC2626'\n",
        "}\n"
      ],
      "metadata": {
        "id": "AFKV_J6aBO8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Severity color mapping\n",
        "severity_colors = {\n",
        "    'Critical': NDIS_COLORS['critical'],\n",
        "    'High': NDIS_COLORS['high'],\n",
        "    'Medium': NDIS_COLORS['medium'],\n",
        "    'Low': NDIS_COLORS['low']"
      ],
      "metadata": {
        "id": "50YV2U__BPn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"NDIS Executive Dashboard\",\n",
        "    page_icon=\"ðŸ“Š\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\""
      ],
      "metadata": {
        "id": "3fGjdg4MBkwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom CSS for NDIS accessible theme\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    /* NDIS Accessible Theme */\n",
        "    :root {\n",
        "        --primary-color: #003F5C;      /* Deep Blue */\n",
        "        --secondary-color: #2F9E7D;    /* Teal/Turquoise */\n",
        "        --accent-color: #F59C2F;       /* Amber/Orange */\n",
        "        --background-color: #F7F9FA;   /* Light Neutral Gray */\n",
        "        --card-background: #FFFFFF;    /* White */\n",
        "        --text-primary: #1B1B1B;       /* Charcoal */\n",
        "        --text-on-dark: #FFFFFF;       /* White */\n",
        "    }\n",
        "\n",
        "    /* Main app styling */\n",
        "    .main > div {\n",
        "        background-color: var(--background-color);\n",
        "        padding-top: 2rem;\n",
        "    }\n",
        "\n",
        "    /* Sidebar styling */\n",
        "    .css-1d391kg {\n",
        "        background-color: var(--primary-color);\n",
        "    }\n",
        "\n",
        "    .css-1d391kg .css-1v0mbdj {\n",
        "        color: var(--text-on-dark);\n",
        "    }\n",
        "\n",
        "    /* Metric cards with NDIS theme */\n",
        "    .metric-card {\n",
        "        background-color: var(--card-background);\n",
        "        padding: 1.5rem;\n",
        "        border-radius: 0.75rem;\n",
        "        border-left: 4px solid var(--secondary-color);\n",
        "        box-shadow: 0 2px 4px rgba(0, 63, 92, 0.1);\n",
        "        margin-bottom: 1rem;\n",
        "    }\n",
        "\n",
        "    .metric-card h4 {\n",
        "        color: var(--primary-color);\n",
        "        font-weight: 600;\n",
        "        margin-bottom: 0.5rem;\n",
        "    }\n",
        "\n",
        "    .metric-card h2 {\n",
        "        color: var(--text-primary);\n",
        "        font-weight: 700;\n",
        "        margin: 0.5rem 0;\n",
        "    }\n",
        "\n",
        "    /* Alert cards */\n",
        "    .alert-card {\n",
        "        background-color: var(--card-background);\n",
        "        padding: 1rem;\n",
        "        border-radius: 0.5rem;\n",
        "        border-left: 4px solid var(--accent-color);\n",
        "        margin-bottom: 0.75rem;\n",
        "        box-shadow: 0 1px 3px rgba(245, 156, 47, 0.1);\n",
        "    }\n",
        "\n",
        "    .critical-alert {\n",
        "        border-left-color: #DC2626;\n",
        "        background-color: #FEF2F2;\n",
        "    }\n",
        "\n",
        "    .success-alert {\n",
        "        border-left-color: var(--secondary-color);\n",
        "        background-color: #F0FDF4;\n",
        "    }\n",
        "\n",
        "    .warning-alert {\n",
        "        border-left-color: var(--accent-color);\n",
        "        background-color: #FFFBEB;\n",
        "    }\n",
        "\n",
        "    /* Headers and titles */\n",
        "    h1, h2, h3 {\n",
        "        color: var(--primary-color) !important;\n",
        "    }\n",
        "\n",
        "    /* Buttons */\n",
        "    .stButton > button {\n",
        "        background-color: var(--secondary-color);\n",
        "        color: var(--text-on-dark);\n",
        "        border: none;\n",
        "        border-radius: 0.5rem;\n",
        "        font-weight: 500;\n",
        "        transition: all 0.3s ease;\n",
        "    }\n",
        "\n",
        "    .stButton > button:hover {\n",
        "        background-color: #267A63;\n",
        "        box-shadow: 0 4px 8px rgba(47, 158, 125, 0.3);\n",
        "    }\n",
        "\n",
        "    /* ML specific styling */\n",
        "    .ml-card {\n",
        "        background-color: var(--card-background);\n",
        "        padding: 1.5rem;\n",
        "        border-radius: 0.75rem;\n",
        "        border: 2px solid var(--accent-color);\n",
        "        box-shadow: 0 4px 8px rgba(245, 156, 47, 0.2);\n",
        "        margin-bottom: 1rem;\n",
        "    }\n",
        "\n",
        "    .prediction-card {\n",
        "        background: linear-gradient(135deg, #F0FDF4 0%, #DCFCE7 100%);\n",
        "        border-left: 4px solid var(--success);\n",
        "    }\n",
        "\n",
        "    .anomaly-card {\n",
        "        background: linear-gradient(135deg, #FEF2F2 0%, #FEE2E2 100%);\n",
        "        border-left: 4px solid var(--error);\n",
        "    }\n",
        "\n",
        "    /* Selectboxes and inputs */\n",
        "    .stSelectbox > div > div {\n",
        "        background-color: var(--card-background);\n",
        "        border: 1px solid var(--secondary-color);\n",
        "        border-radius: 0.5rem;\n",
        "    }\n",
        "\n",
        "    /* Tabs */\n",
        "    .stTabs [data-baseweb=\"tab-list\"] {\n",
        "        background-color: var(--card-background);\n",
        "        border-radius: 0.5rem;\n",
        "    }\n",
        "\n",
        "    .stTabs [data-baseweb=\"tab\"] {\n",
        "        color: var(--primary-color);\n",
        "        font-weight: 500;\n",
        "    }\n",
        "\n",
        "    .stTabs [aria-selected=\"true\"] {\n",
        "        background-color: var(--secondary-color);\n",
        "        color: var(--text-on-dark);\n",
        "    }\n",
        "\n",
        "    /* Metrics */\n",
        "    [data-testid=\"metric-container\"] {\n",
        "        background-color: var(--card-background);\n",
        "        border: 1px solid var(--secondary-color);\n",
        "        border-radius: 0.5rem;\n",
        "        padding: 1rem;\n",
        "        box-shadow: 0 2px 4px rgba(47, 158, 125, 0.1);\n",
        "    }\n",
        "\n",
        "    [data-testid=\"metric-container\"] > div {\n",
        "        color: var(--primary-color);\n",
        "    }\n",
        "\n",
        "    /* Dataframe styling */\n",
        "    .stDataFrame {\n",
        "        background-color: var(--card-background);\n",
        "        border-radius: 0.5rem;\n",
        "        overflow: hidden;\n",
        "    }\n",
        "\n",
        "    /* Progress bars */\n",
        "    .stProgress > div > div {\n",
        "        background-color: var(--secondary-color);\n",
        "    }\n",
        "\n",
        "    /* File uploader */\n",
        "    .stFileUploader {\n",
        "        background-color: var(--card-background);\n",
        "        border: 2px dashed var(--secondary-color);\n",
        "        border-radius: 0.75rem;\n",
        "        padding: 2rem;\n",
        "    }\n",
        "\n",
        "    /* Info, warning, success, error boxes */\n",
        "    .stAlert {\n",
        "        border-radius: 0.5rem;\n",
        "    }\n",
        "\n",
        "    /* Custom status indicators */\n",
        "    .status-indicator {\n",
        "        display: inline-block;\n",
        "        width: 12px;\n",
        "        height: 12px;\n",
        "        border-radius: 50%;\n",
        "        margin-right: 8px;\n",
        "    }\n",
        "\n",
        "    .status-high { background-color: #DC2626; }\n",
        "    .status-medium { background-color: var(--accent-color); }\n",
        "    .status-low { background-color: var(--secondary-color); }\n",
        "    .status-compliant { background-color: var(--secondary-color); }\n",
        "    .status-overdue { background-color: #DC2626; }\n",
        "\n",
        "    /* Card containers */\n",
        "    .dashboard-card {\n",
        "        background-color: var(--card-background);\n",
        "        padding: 1.5rem;\n",
        "        border-radius: 0.75rem;\n",
        "        box-shadow: 0 2px 8px rgba(0, 63, 92, 0.08);\n",
        "        border: 1px solid #E5E7EB;\n",
        "        margin-bottom: 1rem;\n",
        "    }\n",
        "\n",
        "    /* Section headers */\n",
        "    .section-header {\n",
        "        color: var(--primary-color);\n",
        "        border-bottom: 2px solid var(--secondary-color);\n",
        "        padding-bottom: 0.5rem;\n",
        "        margin-bottom: 1rem;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n"
      ],
      "metadata": {
        "id": "ZIdcBx_4Bn02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning Helper Functions\n",
        "@st.cache_data\n",
        "def prepare_ml_features(df):\n",
        "    \"\"\"Prepare features for machine learning models\"\"\"\n",
        "    if df.empty:\n",
        "        return None, None, None\n",
        "\n",
        "    # Create feature dataframe\n",
        "    features_df = df.copy()\n",
        "\n",
        "    # Encode categorical variables\n",
        "    label_encoders = {}\n",
        "    categorical_cols = ['location', 'incident_type', 'contributing_factors', 'reported_by']\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if col in features_df.columns:\n",
        "            le = LabelEncoder()\n",
        "            features_df[f'{col}_encoded'] = le.fit_transform(features_df[col].fillna('Unknown'))\n",
        "            label_encoders[col] = le\n",
        "\n",
        "    # Create numerical features\n",
        "    numerical_features = []\n",
        "    feature_names = []\n",
        ""
      ],
      "metadata": {
        "id": "H50C97WWBxym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Time-based features\n",
        "    if 'incident_date' in features_df.columns:\n",
        "        features_df['day_of_week'] = features_df['incident_date'].dt.dayofweek\n",
        "        features_df['month'] = features_df['incident_date'].dt.month\n",
        "        features_df['hour'] = pd.to_datetime(features_df['incident_time'], format='%H:%M', errors='coerce').dt.hour\n",
        "        numerical_features.extend(['day_of_week', 'month'])\n",
        "        feature_names.extend(['day_of_week', 'month'])\n",
        "\n",
        "        if not features_df['hour'].isna().all():\n",
        "            numerical_features.append('hour')\n",
        "            feature_names.append('hour')\n",
        "\n",
        "    # Encoded categorical features\n",
        "    for col in categorical_cols:\n",
        "        if f'{col}_encoded' in features_df.columns:\n",
        "            numerical_features.append(f'{col}_encoded')\n",
        "            feature_names.append(f'{col}_encoded')\n",
        "\n",
        "    # Other numerical features\n",
        "    if 'reporting_delay_hours' in features_df.columns:\n",
        "        numerical_features.append('reporting_delay_hours')\n",
        "        feature_names.append('reporting_delay_hours')\n",
        "\n",
        "    if 'age_at_incident' in features_df.columns:\n",
        "        numerical_features.append('age_at_incident')\n",
        "        feature_names.append('age_at_incident')\n",
        "\n",
        "    # Create feature matrix\n",
        "    X = features_df[numerical_features].fillna(0)\n",
        "\n",
        "    return X, feature_names, label_encoders"
      ],
      "metadata": {
        "id": "eTszxOShB3EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@st.cache_data\n",
        "def train_severity_prediction_model(df):\n",
        "    \"\"\"Train a model to predict incident severity\"\"\"\n",
        "    if df.empty or len(df) < 20:\n",
        "        return None, None, None\n",
        "\n",
        "    X, feature_names, label_encoders = prepare_ml_features(df)\n",
        "    if X is None:\n",
        "        return None, None, None\n",
        "\n",
        "    # Prepare target variable\n",
        "    severity_map = {'Low': 0, 'Medium': 1, 'High': 2, 'Critical': 3}\n",
        "    y = df['severity'].map(severity_map)\n",
        "\n",
        "    # Remove rows with missing target\n",
        "    mask = ~y.isna()\n",
        "    X = X[mask]\n",
        "    y = y[mask]\n",
        "\n",
        "    if len(X) < 10:\n",
        "        return None, None, None\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Train Random Forest model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return model, accuracy, feature_names"
      ],
      "metadata": {
        "id": "zWKauCrKCNLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@st.cache_data\n",
        "def perform_anomaly_detection(df):\n",
        "    \"\"\"Perform anomaly detection on incidents\"\"\"\n",
        "    if df.empty or len(df) < 10:\n",
        "        return None, None\n",
        "\n",
        "    X, feature_names, _ = prepare_ml_features(df)\n",
        "    if X is None:\n",
        "        return None, None\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Isolation Forest for anomaly detection\n",
        "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "    anomaly_labels = iso_forest.fit_predict(X_scaled)\n",
        "\n",
        "    # One-Class SVM\n",
        "    oc_svm = OneClassSVM(nu=0.1)\n",
        "    svm_labels = oc_svm.fit_predict(X_scaled)\n",
        "\n",
        "    # Combine results\n",
        "    df_with_anomalies = df.copy()\n",
        "    df_with_anomalies['isolation_forest_anomaly'] = anomaly_labels == -1\n",
        "    df_with_anomalies['svm_anomaly'] = svm_labels == -1\n",
        "    df_with_anomalies['anomaly_score'] = iso_forest.decision_function(X_scaled)\n",
        "\n",
        "    return df_with_anomalies, feature_names\n"
      ],
      "metadata": {
        "id": "eb1qBHQNCRqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@st.cache_data\n",
        "def find_association_rules(df):\n",
        "    \"\"\"Find association rules between incident characteristics\"\"\"\n",
        "    if not MLXTEND_AVAILABLE or df.empty or len(df) < 20:\n",
        "        return None, None\n",
        "\n",
        "    # Prepare transaction data\n",
        "    transactions = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        transaction = []\n",
        "\n",
        "        # Add categorical features to transactions\n",
        "        if pd.notna(row['location']):\n",
        "            transaction.append(f\"location_{row['location']}\")\n",
        "        if pd.notna(row['incident_type']):\n",
        "            transaction.append(f\"type_{row['incident_type']}\")\n",
        "        if pd.notna(row['severity']):\n",
        "            transaction.append(f\"severity_{row['severity']}\")\n",
        "        if pd.notna(row['contributing_factors']):\n",
        "            transaction.append(f\"factor_{row['contributing_factors']}\")\n",
        "\n",
        "        # Add binary features\n",
        "        if row.get('medical_attention_required') == 'Yes':\n",
        "            transaction.append('medical_required')\n",
        "        if row.get('reportable') == 'Yes':\n",
        "            transaction.append('reportable')\n",
        "        if row.get('same_day_reporting', False):\n",
        "            transaction.append('same_day_reported')\n",
        "\n",
        "        if transaction:\n",
        "            transactions.append(transaction)\n",
        "\n",
        "    if not transactions:\n",
        "        return None, None\n",
        "\n",
        "    # Create binary matrix\n",
        "    te = TransactionEncoder()\n",
        "    te_ary = te.fit(transactions).transform(transactions)\n",
        "    df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "    # Find frequent itemsets\n",
        "    frequent_itemsets = apriori(df_encoded, min_support=0.1, use_colnames=True)\n",
        "\n",
        "    if frequent_itemsets.empty:\n",
        "        return None, None\n",
        "\n",
        "    # Generate association rules\n",
        "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
        "\n",
        "    return frequent_itemsets, rules"
      ],
      "metadata": {
        "id": "nEP9mYQFCsFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time_series_forecast(df, periods=30):\n",
        "    \"\"\"Perform time series forecasting of incident counts\"\"\"\n",
        "    if not STATSMODELS_AVAILABLE or df.empty:\n",
        "        return None, None\n",
        "\n",
        "    # Aggregate incidents by date\n",
        "    daily_counts = df.groupby(df['incident_date'].dt.date).size().reset_index()\n",
        "    daily_counts.columns = ['date', 'incident_count']\n",
        "    daily_counts['date'] = pd.to_datetime(daily_counts['date'])\n",
        "    daily_counts = daily_counts.set_index('date').sort_index()\n",
        "\n",
        "    # Ensure we have enough data\n",
        "    if len(daily_counts) < 30:\n",
        "        return None, None\n",
        "\n",
        "    # Fill missing dates with 0\n",
        "    date_range = pd.date_range(start=daily_counts.index.min(), end=daily_counts.index.max(), freq='D')\n",
        "    daily_counts = daily_counts.reindex(date_range, fill_value=0)\n",
        "\n",
        "    try:\n",
        "        # Exponential Smoothing forecast\n",
        "        model = ExponentialSmoothing(daily_counts['incident_count'],\n",
        "                                   trend='add',\n",
        "                                   seasonal=None)\n",
        "        fitted_model = model.fit()\n",
        "        forecast = fitted_model.forecast(periods)\n",
        "\n",
        "        # Create forecast dates\n",
        "        forecast_dates = pd.date_range(start=daily_counts.index.max() + pd.Timedelta(days=1),\n",
        "                                     periods=periods, freq='D')\n",
        "\n",
        "        forecast_df = pd.DataFrame({\n",
        "            'date': forecast_dates,\n",
        "            'forecast': forecast\n",
        "        })\n",
        "\n",
        "        return daily_counts, forecast_df\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Forecasting error: {str(e)}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "fGtBTvTlC2Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@st.cache_data\n",
        "def load_incident_data():\n",
        "    \"\"\"Load and prepare the actual NDIS incident data\"\"\"\n",
        "    try:\n",
        "        # Try multiple possible file paths\n",
        "        possible_paths = [\n",
        "            'text data/ndis_incidents_synthetic.csv',  # GitHub repo structure\n",
        "            'ndis_incidents_synthetic.csv',\n",
        "            './ndis_incidents_synthetic.csv',\n",
        "            'data/ndis_incidents_synthetic.csv',\n",
        "            '../ndis_incidents_synthetic.csv',\n",
        "            './text data/ndis_incidents_synthetic.csv'\n",
        "        ]\n",
        "\n",
        "        df = None\n",
        "        for path in possible_paths:\n",
        "            try:\n",
        "                df = pd.read_csv(path)\n",
        "                st.sidebar.success(f\"âœ… Data loaded from: {path}\")\n",
        "                break\n",
        "            except FileNotFoundError:\n",
        "                continue\n",
        "\n",
        "        if df is None:\n",
        "            # If no file found, show file upload option\n",
        "            st.error(\"CSV file not found. Please upload your data file below.\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "SZeTIxT3C7eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Clean and prepare the data\n",
        "        df['incident_date'] = pd.to_datetime(df['incident_date'], format='%d/%m/%Y', errors='coerce')\n",
        "        df['notification_date'] = pd.to_datetime(df['notification_date'], format='%d/%m/%Y', errors='coerce')\n",
        "        df['dob'] = pd.to_datetime(df['dob'], format='%d/%m/%Y', errors='coerce')\n",
        "\n",
        "        # Calculate reporting delay in hours\n",
        "        df['reporting_delay_hours'] = (df['notification_date'] - df['incident_date']).dt.total_seconds() / 3600\n",
        "        df['same_day_reporting'] = df['reporting_delay_hours'] <= 24\n",
        "\n",
        "        # Calculate age at incident\n",
        "        df['age_at_incident'] = (df['incident_date'] - df['dob']).dt.days / 365.25\n",
        "\n",
        "        # Add month names for seasonal analysis\n",
        "        df['incident_month'] = df['incident_date'].dt.month_name()\n",
        "        df['incident_year'] = df['incident_date'].dt.year\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading data: {str(e)}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "_hTjSv8PC_T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sample_data():\n",
        "    \"\"\"Create sample NDIS incident data for demonstration\"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Sample data matching your CSV structure\n",
        "    sample_data = {\n",
        "        'incident_id': [f'INC-2024-{i:04d}' for i in range(1, 501)],  # Increased to 500 for better ML\n",
        "        'participant_name': [f'Participant {i}' for i in range(1, 501)],\n",
        "        'ndis_number': np.random.randint(400000000, 500000000, 500),\n",
        "        'dob': pd.date_range('1950-01-01', '2010-12-31', periods=500).strftime('%d/%m/%Y'),\n",
        "        'incident_date': pd.date_range('2023-01-01', '2024-12-31', periods=500).strftime('%d/%m/%Y'),\n",
        "        'incident_time': [f'{np.random.randint(0,24):02d}:{np.random.randint(0,60):02d}' for _ in range(500)],\n",
        "        'notification_date': pd.date_range('2023-01-01', '2024-12-31', periods=500).strftime('%d/%m/%Y'),\n",
        "        'location': np.random.choice(['Group Home', 'Transport Vehicle', 'Day Program', 'Community Access', 'Therapy Clinic'], 500),\n",
        "        'incident_type': np.random.choice(['Injury', 'Missing Person', 'Death', 'Restrictive Practices', 'Transport Incident', 'Medication Error'], 500),\n",
        "        'subcategory': np.random.choice(['Fall', 'Unexplained absence', 'Natural causes', 'Unauthorised', 'Vehicle crash', 'Wrong dose'], 500),\n",
        "        'severity': np.random.choice(['Critical', 'High', 'Medium', 'Low'], 500, p=[0.1, 0.2, 0.4, 0.3]),\n",
        "        'reportable': np.random.choice(['Yes', 'No'], 500, p=[0.7, 0.3]),\n",
        "        'description': ['Sample incident description' for _ in range(500)],\n",
        "        'immediate_action': ['Immediate action taken' for _ in range(500)],\n",
        "        'actions_taken': ['Follow-up actions completed' for _ in range(500)],\n",
        "        'contributing_factors': np.random.choice(['Staff error', 'Equipment failure', 'Environmental factors', 'Participant behavior', 'System failure'], 500),\n",
        "        'reported_by': [f'Staff Member {i} (Support Worker)' for i in range(1, 501)],\n",
        "        'injury_type': np.random.choice(['No physical injury', 'Minor injury', 'Major injury'], 500, p=[0.6, 0.3, 0.1]),\n",
        "        'injury_severity': np.random.choice(['None', 'Mild', 'Moderate', 'Severe'], 500, p=[0.5, 0.3, 0.15, 0.05]),\n",
        "        'treatment_required': np.random.choice(['Yes', 'No'], 500, p=[0.3, 0.7]),\n",
        "        'medical_attention_required': np.random.choice(['Yes', 'No'], 500, p=[0.25, 0.75]),\n",
        "        'medical_treatment_type': np.random.choice(['None', 'First aid', 'GP visit', 'Hospital'], 500, p=[0.6, 0.25, 0.1, 0.05]),\n",
        "        'medical_outcome': np.random.choice(['No treatment required', 'Treated and released', 'Ongoing monitoring'], 500, p=[0.7, 0.25, 0.05])\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(sample_data)"
      ],
      "metadata": {
        "id": "qqz9ez_ODFVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data with fallback options\n",
        "df = load_incident_data()\n"
      ],
      "metadata": {
        "id": "kF9FIHFJDJXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If no data loaded, offer file upload and sample data options\n",
        "if df is None:\n",
        "    st.title(\"ðŸ¥ NDIS Dashboard - Data Loading\")\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        st.subheader(\"ðŸ“ Upload Your Data\")\n",
        "        uploaded_file = st.file_uploader(\n",
        "            \"Choose your NDIS incidents CSV file\",\n",
        "            type=['csv'],\n",
        "            help=\"Upload your ndis_incidents_synthetic.csv file or any CSV with the same structure\"\n",
        "        )\n",
        "\n",
        "        if uploaded_file is not None:\n",
        "            try:\n",
        "                df = pd.read_csv(uploaded_file)\n",
        "\n",
        "                # Apply the same data processing\n",
        "                df['incident_date'] = pd.to_datetime(df['incident_date'], format='%d/%m/%Y', errors='coerce')\n",
        "                df['notification_date'] = pd.to_datetime(df['notification_date'], format='%d/%m/%Y', errors='coerce')\n",
        "                df['dob'] = pd.to_datetime(df['dob'], format='%d/%m/%Y', errors='coerce')\n",
        "                df['reporting_delay_hours'] = (df['notification_date'] - df['incident_date']).dt.total_seconds() / 3600\n",
        "                df['same_day_reporting'] = df['reporting_delay_hours'] <= 24\n",
        "                df['age_at_incident'] = (df['incident_date'] - df['dob']).dt.days / 365.25\n",
        "                df['incident_month'] = df['incident_date'].dt.month_name()\n",
        "                df['incident_year'] = df['incident_date'].dt.year\n",
        "\n",
        "                st.success(f\"âœ… Successfully loaded {len(df)} incidents from uploaded file!\")\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error processing uploaded file: {str(e)}\")\n",
        "                df = None"
      ],
      "metadata": {
        "id": "qDhYDKiJDL79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  with col2:\n",
        "        st.subheader(\"ðŸŽ¯ Use Sample Data\")\n",
        "        st.info(\"\"\"\n",
        "        Can't find your CSV file? Use our enhanced sample data to explore the dashboard features.\n",
        "\n",
        "        The sample data includes:\n",
        "        - 500 realistic NDIS incidents\n",
        "        - All required fields and categories\n",
        "        - Proper date formatting\n",
        "        - Enhanced for machine learning\n",
        "        \"\"\")\n",
        "\n",
        "        if st.button(\"ðŸš€ Load Sample Data\"):\n",
        "            df = create_sample_data()\n",
        "\n",
        "            # Apply the same data processing\n",
        "            df['incident_date'] = pd.to_datetime(df['incident_date'], format='%d/%m/%Y', errors='coerce')\n",
        "            df['notification_date'] = pd.to_datetime(df['notification_date'], format='%d/%m/%Y', errors='coerce')\n",
        "            df['dob'] = pd.to_datetime(df['dob'], format='%d/%m/%Y', errors='coerce')\n",
        "            df['reporting_delay_hours'] = (df['notification_date'] - df['incident_date']).dt.total_seconds() / 3600\n",
        "            df['same_day_reporting'] = df['reporting_delay_hours'] <= 24\n",
        "            df['age_at_incident'] = (df['incident_date'] - df['dob']).dt.days / 365.25\n",
        "            df['incident_month'] = df['incident_date'].dt.month_name()\n",
        "            df['incident_year'] = df['incident_date'].dt.year\n",
        "\n",
        "            st.success(\"âœ… Sample data loaded successfully!\")\n",
        "            st.rerun()\n",
        "\n",
        "    st.stop()\n",
        "\n",
        "# Sidebar for navigation and filters\n",
        "st.sidebar.title(\"ðŸ¥ NDIS Dashboard\")\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "# Navigation\n",
        "page = st.sidebar.selectbox(\n",
        "    \"Dashboard Pages\",\n",
        "    [\"Executive Summary\", \"Operational Performance\", \"Compliance & Investigation\", \"ðŸ¤– Machine Learning Analytics\", \"Risk Analysis\"]\n",
        ")\n",
        "\n",
        "# Filters\n",
        "st.sidebar.markdown(\"### Filters\")\n",
        "\n",
        "# Date range filter\n",
        "min_date = df['incident_date'].min()\n",
        "max_date = df['incident_date'].max()\n",
        "date_range = st.sidebar.date_input(\n",
        "    \"Date Range\",\n",
        "    value=(min_date, max_date),\n",
        "    min_value=min_date,\n",
        "    max_value=max_date\n",
        ")\n",
        "\n",
        "# Location filter\n",
        "locations = ['All'] + sorted(df['location'].dropna().unique().tolist())\n",
        "selected_location = st.sidebar.selectbox(\"Location\", locations)\n",
        "\n",
        "# Severity filter\n",
        "severities = st.sidebar.multiselect(\n",
        "    \"Severity\",\n",
        "    df['severity'].dropna().unique().tolist(),\n",
        "    default=df['severity'].dropna().unique().tolist()\n",
        ")\n",
        "\n",
        "# Incident type filter\n",
        "incident_types = st.sidebar.multiselect(\n",
        "    \"Incident Type\",\n",
        "    df['incident_type'].dropna().unique().tolist(),\n",
        "    default=df['incident_type'].dropna().unique().tolist()\n",
        ")\n",
        "\n",
        "# Apply filters\n",
        "filtered_df = df.copy()\n",
        "\n",
        "if len(date_range) == 2:\n",
        "    filtered_df = filtered_df[\n",
        "        (filtered_df['incident_date'] >= pd.Timestamp(date_range[0])) &\n",
        "        (filtered_df['incident_date'] <= pd.Timestamp(date_range[1]))\n",
        "    ]\n",
        "\n",
        "if selected_location != 'All':\n",
        "    filtered_df = filtered_df[filtered_df['location'] == selected_location]\n",
        "\n",
        "if severities:\n",
        "    filtered_df = filtered_df[filtered_df['severity'].isin(severities)]\n",
        "\n",
        "if incident_types:\n",
        "    filtered_df = filtered_df[filtered_df['incident_type'].isin(incident_types)]\n"
      ],
      "metadata": {
        "id": "6JNQmeQgDTi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main dashboard content\n",
        "if page == \"Executive Summary\":\n",
        "    st.title(\"ðŸ“Š NDIS Executive Dashboard\")\n",
        "    st.markdown(\"**Strategic Overview - Incident Analysis & Risk Management**\")\n",
        "    st.markdown(f\"*Showing {len(filtered_df)} incidents from {len(df)} total records*\")\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # Key metrics row\n",
        "    col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "    total_incidents = len(filtered_df)\n",
        "    critical_incidents = len(filtered_df[filtered_df['severity'] == 'Critical'])\n",
        "    same_day_rate = filtered_df['same_day_reporting'].mean() * 100 if len(filtered_df) > 0 else 0\n",
        "    reportable_rate = (filtered_df['reportable'] == 'Yes').mean() * 100 if len(filtered_df) > 0 else 0\n",
        "\n",
        "    with col1:\n",
        "        st.markdown(f\"\"\"\n",
        "        <div class=\"metric-card\">\n",
        "            <h4>Total Incidents</h4>\n",
        "            <h2>{total_incidents}</h2>\n",
        "            <p style=\"color: {ND}"
      ],
      "metadata": {
        "id": "yXRbMiyODX8-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}